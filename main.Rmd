# Installing Required Packages
```{r}
# install required packages/library
install.packages("matlib")
install.packages("rsample")
install.packages("ggplot2")
install.packages("corrplot")
install.packages("MASS")
install.packages("gridExtra")
install.packages("dplyr")
install.packages("knitr")
```

# Loading Libraries
```{r}
# import required packages/libraries
library(matlib)     # for matrix operations
library(ggplot2)    # for data visualization
library(rsample)    # for data splitting
library(corrplot)   # for correlation visualization
library(MASS)       # for statistical functions
library(gridExtra)  # for arranging plots
library(dplyr)      # for data manipulation
library(knitr)      # for tables
```

# Loading Datasets
```{r}
# load features dataset (independent variable)
features <- as.matrix(read.csv(file="data/features.csv", header=FALSE))
colnames(features) <- c("x1", "x3", "x4", "x5")

# load target dataset (dependent variable)
target <- as.matrix(read.csv(file="data/target.csv", header=FALSE))
colnames(target) <- c("X2")

# load time series dataset
time <- as.matrix((read.csv(file="data/timeseries.csv", header=FALSE)))
colnames(time) <- c("T1")

# display the first few rows of each dataset
cat("Features Dataset: \n")
head(features)

cat("\nTarget Dataset: \n")
head(target)

cat("\nTimeseries Dataset: \n")
head(features)
```

# Summary Statistics
```{r}
# create a combined dataframe
combined_df <- data.frame(
  Temperature = features[,"x1"],
  Ambient_Pressure = features[,"x3"],
  Relative_Humidity = features[,"x4"],
  Exhaust_Vacuum = features[,"x5"],
  Energy_Output = target[,"X2"]
)

# summary statistics
summary_stats <- summary(combined_df)
print(summary_stats)

# additional statistics
additional_stats <- data.frame(
  Standard_Deviation = sapply(combined_df, sd),
  IQR = sapply(combined_df, IQR),
  Skewness = sapply(combined_df, function(x) {
    (mean(x) - median(x)) / sd(x)
  })
)

kable(additional_stats, caption = "Additional Statistics for Each Variable")
```

# Task 1: Preliminary Data Analysis
## Time Series Analysis
### Time Series Analysis (Input Signal)
```{r}
# convert features into time series objects
features.ts <- ts(features, start = c(min(time), max(time)), frequency = 1)

# create individual time series plots for each input signal
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# plot for temperature (x1)
plot(features.ts[,"x1"], main = "Time Series - Temperature",
     xlab = "Time", ylab = "Temperature (°C)", col = "darkred")

# plot for ambient pressure (x3)
plot(features.ts[,"x3"], main = "Time Series - Ambient Pressure",
     xlab = "Time", ylab = "Pressure (millibar)", col = "darkblue")

# plot for relative humidity (x4)
plot(features.ts[,"x4"], main = "Time Series - Relative Humidity",
     xlab = "Time", ylab = "Humidity (%)", col = "darkgreen")

# plot for exhaust vacuum (x5)
plot(features.ts[,"x5"], main = "Time Series - Exhaust Vacuum",
     xlab = "Time", ylab = "Vacuum (cm Hg)", col = "darkorange")

# reset layout
par(mfrow = c(1, 1))
```

### Time Series Analysis (Input Signal)
```{r}
# convert target to time series object
target.ts <- ts(target, start = c(min(time), max(time)), frequency = 1)

# plot output signal time series
plot(target.ts, main = "Time Series - Net Hourly Electrical Energy Output",
     xlab = "Time", ylab = "Energy Output (MW)", col = "purple", lwd = 1.5)
```

## Distribution of Each Signal (Output Signal)
```{r}
# function to create enhanced histogram with density plot and normal reference
plot_enhanced_histogram <- function(data, column, title, xlab) {
  # calculate density
  dens <- density(data)
  
  # create histogram
  hist(data, freq = FALSE, main = title, xlab = xlab, col = "lightblue",
    border = "white", ylim = c(0, max(dens$y) * 1.1))
  
  # add density line
  lines(dens, lwd = 2, col = "darkblue")
  
  # add normal distribution reference
  curve(dnorm(x, mean = mean(data), sd = sd(data)), add = TRUE,
    col = "red", lty = 2, lwd = 1.5)
  
  # add legend
  legend("topleft", legend = c("Density", "Normal Reference"),
    col = c("darkblue", "red"), lty = c(1, 2), lwd = c(2, 1.5))
  
  # add mean line
  abline(v = mean(data), col = "darkgreen", lwd = 1.5, lty = 2)
  
  # add text for mean and standard deviation
  usr <- par("usr")  # c(x1, x2, y1, y2)
  text(x = usr[2] - 0.02 * diff(usr[1:2]), y = usr[4] - 0.05 * diff(usr[3:4]),  
    labels = sprintf("Mean: %.2f\nSD:   %.2f", mean(data), sd(data)), adj = c(1, 1))
}

# set up plotting area
par(mfrow = c(3, 2), mar = c(4, 4, 3, 1))

# histogram for temperature (x1)
plot_enhanced_histogram(
  features[,"x1"], 
  "x1", 
  "Distribution of Temperature (x1)", 
  "Ambient temperature (°C)"
)

# histogram for ambient pressure (x3)
plot_enhanced_histogram(
  features[,"x3"], 
  "x3", 
  "Distribution of Ambient Pressure (x3)", 
  "Atmospheric pressure (millibar)"
)

# histogram for relative humidity (x4)
plot_enhanced_histogram(
  features[,"x4"], 
  "x4", 
  "Distribution of Relative Humidity (x4)", 
  "Humidity level (%)"
)

# histogram for exhaust vacuum (x5)
plot_enhanced_histogram(
  features[,"x5"], 
  "x5", 
  "Distribution of Exhaust Vacuum (x5)", 
  "Vacuum (cm Hg)"
)

# histogram for net hourly electrical energy output (X2)
plot_enhanced_histogram(
  target[,"X2"], 
  "X2", 
  "Distribution of Net Hourly Electrical Energy Output (X2)", 
  "Net hourly electrical energy output (MW)"
)

# reset layout to single plot
par(mfrow = c(1, 1))
```

## Correlation Analysis and Scatter Plots
### Correlation Matrix
```{r}
# combine features and target for correlation analysis
combined <- cbind(features, target)

# calculate correlation matrix
correlation <- cor(combined)
print(correlation)

# create enhanced correlation plot
corrplot(correlation, method = "color", type = "upper", 
         addCoef.col = "black", number.cex = 0.7,
         tl.col = "black", tl.srt = 45,
         title = "Correlation Matrix of Power Plant Variables",
         mar = c(0, 0, 1, 0))
```

### Scatter Plots
```{r}
# Define a palette of 4 colors (one per feature)
base_colors <- c("steelblue", "tomato", "forestgreen", "orchid")
# Optionally add a bit of transparency
plot_colors <- sapply(base_colors, function(col) adjustcolor(col, alpha.f = 0.7))

# List of columns and their labels
plots <- list(
  x1 = c("Ambient Temp (°C)",       "Net Energy Output (MW)"),
  x3 = c("Ambient Pressure (mbar)", "Net Energy Output (MW)"),
  x4 = c("Relative Humidity (%)",   "Net Energy Output (MW)"),
  x5 = c("Exhaust Vacuum (cm Hg)",  "Net Energy Output (MW)")
)

# Set up 2 rows × 3 columns
par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))

# Loop over each feature → plot(feature, target) with its own color
i <- 1
for (name in names(plots)) {
  labs <- plots[[name]]
  x    <- features[, name]
  y    <- target[, "X2"]
  
  plot(
    x, y,
    xlab  = labs[1],
    ylab  = labs[2],
    main  = paste(name, "vs X2"),
    pch   = 16,
    cex   = 0.6,
    col   = plot_colors[i]
  )
  i <- i + 1
}

# Leave the last panel empty
plot.new()

# Reset to single plot
par(mfrow = c(1, 1))

```

# Task 2: Regression - Modelling the Between the Gene Expression
```{r}
# candidate nonlinear polynomial regression models
model1<-function(X, theta){
  theta[1]*X$x4 + theta[2]*X$x3^2 + theta[3]
}

model2<-function(X, theta){
  theta[1]*X$x4 + theta[2]*X$x3^2 + theta[3]*X$x5 + theta[4]
}

model3<-function(X, theta){
  theta[1]*X$x3 + theta[2]*X$x4 + theta[3]*X$x5^3
}

model4<-function(X, theta){
  theta[1]*X$x4 + theta[2]*X$x3^2 + theta[3]*X$x5^3 + theta[4]
}

model5<-function(X, theta){
  theta[1]*X$x4 + theta[2]*X$x1^2 + theta[3]*X$x3^2 + theta[4]
}
```

# Task 2.1: estimation of model parameters (theta) for every candidate model using least square
```{r}
# convert matrix into dataframe
X<-as.data.frame(features)

theta_ridge<-function(X, y, lambda=1e-1){
  X_bias<-cbind(1, as.matrix(X))
  XtX <- t(X_bias) %*% X_bias  # compute X^T * X
  XtX_reg <- XtX + lambda * diag(c(0, rep(1, ncol(X)))) # regularization term 
  theta <- ginv(XtX_reg) %*% t(X_bias) %*% y # generalized inverse to solve
  return(theta)
}
```

```{r}
# theta for model 1
X_model1 <- data.frame(x4 = X$x4, x3 = X$x3^2)
theta_model1 <- theta_ridge(X_model1, target)
print("Theta Model 1:")
print(theta_model1)
```

```{r}
# theta for model 2
X_model2 <- data.frame(x4 = X$x4, x3 = X$x3^2, x5 = X$x5)
theta_model2 <- theta_ridge(X_model2, target)
print("Theta Model 2:")
print(theta_model2)
```

```{r}
X_model3 <- data.frame(x3 = X$x3, x4 = X$x4, x5 = X$x5^3)  
theta_model3 <- theta_ridge(X_model3, target)
print("Theta for Model 3:")
print(theta_model3)

```

```{r}
X_model4 <- data.frame(x4 = X$x4, x3 = X$x3^2, x5 = X$x5^3) 
theta_model4 <- theta_ridge(X_model4, target)
print("Theta for Model 4:")
print(theta_model4)
```

```{r}
X_model5 <- data.frame(x4 = X$x4, x1 = X$x1^2, x3 = X$x3^2)
theta_model5 <- theta_ridge(X_model5, target)
print("Theta for Model 5:")
print(theta_model5)
```

# Task 2.2: Model Residual Sum of Squared Error (RSS)
```{r}
compute_rss<-function(X,y,theta,model){
  y_pred<-model(X, theta) 
  rss<-sum((y-y_pred)^2)
  return(rss)
}
```

```{r}
# RSS for model1
rss_model1 <- compute_rss(X_model1, target, theta_model1, model1)
print("Model 1 RSS:")
print(rss_model1)
```

```{r}
rss_model2<-compute_rss(X_model2, target, theta_model2, model2)
print("Model 2 RSS:")
print(rss_model2)
```

```{r}
rss_model3<-compute_rss(X_model3, target, theta_model3, model3)
print("Model 3 RSS:")
print(rss_model3)
```

```{r}
rss_model4<-compute_rss(X_model4, target, theta_model4, model4)
print("Model 4 RSS:")
print(rss_model4)
```

```{r}
rss_model5<-compute_rss(X_model5,target,theta_model5,model5)
print("Model 5 RSS:")
print(rss_model5)
```

# Log-likelihood function
```{r}
compute_loglikelihood_variance<-function(rss, n){
  sigma_squared <- rss/(n-1)
  log_likelihood<- -(n/2)*log(2*pi)-(n/2)*log(sigma_squared)-(1/(2*sigma_squared))*rss
  return(list(log_likelihood=log_likelihood,variance=sigma_squared))
}
```

```{r}
# log likelihood and variance model1
compute_model1<-compute_loglikelihood_variance(rss_model1, length(target))
model1_log_likelihood<-compute_model1$log_likelihood
model1_variance<-compute_model1$variance
print("Model 1 Log-Likelihood:")
print(model1_log_likelihood)
print("Model 1 Variance:")
print(model1_variance)
```

```{r}
# log likelihood and variance model2
compute_model2<-compute_loglikelihood_variance(rss_model2, length(target))
model2_log_likelihood<-compute_model2$log_likelihood
model2_variance<-compute_model2$variance
print("Model 2 Log-Likelihood:")
print(model2_log_likelihood)
print("Model 2 Variance:")
print(model2_variance)
```

```{r}
# log likelihood and variance model3
compute_model3<-compute_loglikelihood_variance(rss_model3, length(target))
model3_log_likelihood<-compute_model3$log_likelihood
model3_variance<-compute_model3$variance
print("Model 3 Log-Likelihood:")
print(model3_log_likelihood)
print("Model 3 Variance:")
print(model3_variance)
```

```{r}
# log likelihood and variance model4
compute_model4<-compute_loglikelihood_variance(rss_model4, length(target))
model4_log_likelihood<-compute_model4$log_likelihood
model4_variance<-compute_model4$variance
print("Model 4 Log-Likelihood:")
print(model4_log_likelihood)
print("Model 4 Variance:")
print(model4_variance)
```

```{r}
# log likelihood and variance model5
compute_model5<-compute_loglikelihood_variance(rss_model5, length(target))
model5_log_likelihood<-compute_model5$log_likelihood
model5_variance<-compute_model5$variance
print("Model 5 Log-Likelihood:")
print(model5_log_likelihood)
print("Model 5 Variance")
print(model5_variance)
```

# Compute AIC and BIC
```{r}
# calculate aic
compute_aic<-function(log_likelihood, k){
  aic<-2 * k - 2 * log_likelihood
  return(aic)
}

# calculate bic
compute_bic<-function(log_likelihood, k, n){
  bic <- k * log(n) - 2 * log_likelihood
  return(bic)
}
```

```{r}
# aic and bic model 1
model1_k<-length(theta_model1)
model1_aic<-compute_aic(model1_log_likelihood, model1_k)
model1_bic<-compute_bic(model1_log_likelihood, model1_k, length(target))
print("Model 1 AIC:")
print(model1_aic)
print("Model 1 BIC:")
print(model1_bic)
```

```{r}
# aic and bic model 2
model2_k<-length(theta_model2)
model2_aic<-compute_aic(model2_log_likelihood, model2_k)
model2_bic<-compute_bic(model2_log_likelihood, model2_k, length(target))
print("Model 2 AIC:")
print(model2_aic)
print("Model 2 BIC:")
print(model2_bic)
```

```{r}
# aic and bic model 3
model3_k<-length(theta_model3)
model3_aic<-compute_aic(model3_log_likelihood, model3_k)
model3_bic<-compute_bic(model3_log_likelihood, model3_k, length(target))
print("Model 3 AIC:")
print(model3_aic)
print("Model 3 BIC:")
print(model3_bic)
```

```{r}
# aic and bic model 4
model4_k<-length(theta_model4)
model4_aic<-compute_aic(model4_log_likelihood, model4_k)
model4_bic<-compute_bic(model4_log_likelihood, model4_k, length(target))
print("Model 4 AIC:")
print(model4_aic)
print("Model 4 BIC:")
print(model4_bic)
```

```{r}
# aic and bic model 5
model5_k<-length(theta_model5)
model5_aic<-compute_aic(model5_log_likelihood, model5_k)
model5_bic<-compute_bic(model5_log_likelihood, model5_k, length(target))
print("Model 5 AIC:")
print(model5_aic)
print("Model 5 BIC:")
print(model5_bic)
```

# Task 2.5: Distributin of model prediction errors
```{r}
compute_model_prediction_error<-function(X, y, theta, model){
  y_pred<-model(X, theta)
  prediction_error<-y - y_pred
  return(prediction_error)
}
```

```{r}
# QQ plot for model 1 distribution
prediction_error_model1 <- compute_model_prediction_error(X_model1, target, theta_model1, model1)
qqnorm(prediction_error_model1, main = "Q-Q Plot of Prediction Error for Model 1")
qqline(prediction_error_model1)  
```

```{r}
# QQ plot for model 2 distribution
prediction_error_model2 <- compute_model_prediction_error(X_model2, target, theta_model2, model2)
qqnorm(prediction_error_model2, main = "Q-Q Plot of Prediction Error for Model 2")
qqline(prediction_error_model2)  
```

```{r}
# QQ plot for model 3 distribution
prediction_error_model3 <- compute_model_prediction_error(X_model3, target, theta_model3, model3)
qqnorm(prediction_error_model3, main = "Q-Q Plot of Prediction Error for Model 3")
qqline(prediction_error_model3)  
```

```{r}
# QQ plot for model 4 distribution
prediction_error_model4 <- compute_model_prediction_error(X_model4, target, theta_model4, model4)
qqnorm(prediction_error_model4, main = "Q-Q Plot of Prediction Error for Model 4")
qqline(prediction_error_model4)  
```

```{r}
# QQ plot for model 5 distribution
prediction_error_model5 <- compute_model_prediction_error(X_model5, target, theta_model5, model5)
qqnorm(prediction_error_model5, main = "Q-Q Plot of Prediction Error for Model 5")
qqline(prediction_error_model5)  
```

# Task 2.7 train Test Split
```{r}
set.seed(42)  # for reproducibility

# Create the full design matrix for Model 5
X_model5 <- data.frame(
  x4 = X$x4,
  x1 = X$x1^2,
  x3 = X$x3^2
)
y <- target

# --- 1. Split into training and testing sets (70/30) ---
n <- nrow(X_model5)
train_idx <- sample(1:n, size = round(0.7 * n))
test_idx <- setdiff(1:n, train_idx)

X_train <- X_model5[train_idx, ]
y_train <- y[train_idx, , drop = FALSE]
X_test <- X_model5[test_idx, ]
y_test <- y[test_idx, , drop = FALSE]

# --- 2. Estimate theta on training data ---
library(MASS)

theta_ridge <- function(X, y, lambda = 1e-1) {
  X_bias <- cbind(1, as.matrix(X))
  XtX <- t(X_bias) %*% X_bias
  XtX_reg <- XtX + lambda * diag(c(0, rep(1, ncol(X))))
  theta <- ginv(XtX_reg) %*% t(X_bias) %*% y
  return(theta)
}

theta_hat <- theta_ridge(X_train, y_train)

# --- 3. Make predictions on the test set ---
X_test_bias <- cbind(1, as.matrix(X_test))
y_pred <- X_test_bias %*% theta_hat

# --- 4. Compute 95% confidence intervals ---
# Residual standard error from training data
X_train_bias <- cbind(1, as.matrix(X_train))
y_train_pred <- X_train_bias %*% theta_hat
residuals_train <- y_train - y_train_pred
rss <- sum(residuals_train^2)
df <- nrow(X_train) - length(theta_hat)
sigma2 <- rss / df

# Standard errors of predictions
XtX_inv <- ginv(t(X_train_bias) %*% X_train_bias + 1e-1 * diag(c(0, rep(1, ncol(X_train)))))
se_pred <- sqrt(diag(X_test_bias %*% XtX_inv %*% t(X_test_bias)) * sigma2)

# 95% confidence intervals
lower_bound <- y_pred - 1.96 * se_pred
upper_bound <- y_pred + 1.96 * se_pred

# --- 5. Plot predictions, confidence intervals, and actual values ---
plot(y_test, type = 'p', col = 'black', pch = 16,
     ylab = "Target", xlab = "Test Sample Index", main = "Model 5 Prediction with 95% CI")
points(y_pred, col = 'blue', pch = 17)
arrows(1:length(y_pred), lower_bound, 1:length(y_pred), upper_bound, 
       length = 0.05, angle = 90, code = 3, col = 'red')

legend("topleft", legend = c("True y", "Predicted y", "95% CI"), 
       col = c("black", "blue", "red"), pch = c(16, 17, NA), lty = c(NA, NA, 1))

```

```{r}
install.packages("gridExtra")
```


```{r}
set.seed(42)

# --- 1) Prepare ---

# Your estimated coefficients from Task 2 (ridge regression)
theta_hat_vec <- as.vector(theta_hat)  # includes intercept + 3 params
names(theta_hat_vec) <- c("intercept", colnames(X_train))

# Find indices of 2 largest absolute coefficients (excluding intercept)
abs_coefs <- abs(theta_hat_vec[-1])  
top2_idx <- order(abs_coefs, decreasing = TRUE)[1:2]
param_names <- names(abs_coefs)[top2_idx]

cat("Selected parameters for ABC:", param_names, "\n")

# Fix other parameters at estimated values
fixed_params <- theta_hat_vec[-c(1, top2_idx + 1)]  # exclude intercept and top2

# --- 2) Define simulation function based on model ---

simulate_y <- function(X_data, theta_vec, sigma_sq) {
  # theta_vec = c(intercept, coef_x4, coef_x1_squared, coef_x3_squared)
  mu <- theta_vec[1] + 
        theta_vec[2] * X_data$x4 + 
        theta_vec[3] * X_data$x1 + 
        theta_vec[4] * X_data$x3
  y_sim <- rnorm(nrow(X_data), mean = mu, sd = sqrt(sigma_sq))
  return(y_sim)
}

# --- 3) Define prior sampler for the 2 params ---

# Get estimated values for the two parameters
theta_1_hat <- theta_hat_vec[top2_idx[1] + 1]  # +1 for intercept offset
theta_2_hat <- theta_hat_vec[top2_idx[2] + 1]

# Define ±10% prior range around estimated values
prior_range_1 <- c(theta_1_hat * 0.9, theta_1_hat * 1.1)
prior_range_2 <- c(theta_2_hat * 0.9, theta_2_hat * 1.1)

# Use estimated sigma_sq from training residuals
sigma_sq_hat <- sigma2

prior_sampler <- function() {
  theta_1 <- runif(1, min = min(prior_range_1), max = max(prior_range_1))
  theta_2 <- runif(1, min = min(prior_range_2), max = max(prior_range_2))
  list(theta_1 = theta_1, theta_2 = theta_2, sigma_sq = sigma_sq_hat)
}

# --- 4) Calculate summary statistics ---

calc_summary_stats <- function(y_vec) {
  c(mean = mean(y_vec), var = var(y_vec))
}

S_obs <- calc_summary_stats(y_train)

# --- 5) Distance metric ---

distance_metric <- function(S_sim, S_obs) {
  sqrt(sum((S_sim - S_obs)^2))
}

# --- 6) ABC rejection ---

N_sim <- 10000  # number of simulations (increase for better posterior)

pilot_runs <- 1000  # for epsilon selection
pilot_distances <- numeric(pilot_runs)

cat("Pilot run to select epsilon...\n")
for (i in 1:pilot_runs) {
  sample_params <- prior_sampler()
  
  # Build full theta vector: intercept + top2 params + fixed params
  theta_full <- numeric(length(theta_hat_vec))
  theta_full[1] <- theta_hat_vec[1]  # intercept fixed
  theta_full[top2_idx[1] + 1] <- sample_params$theta_1
  theta_full[top2_idx[2] + 1] <- sample_params$theta_2
  
  # fixed params fill others
  fixed_names <- names(fixed_params)
  for (name in fixed_names) {
    pos <- which(names(theta_hat_vec) == name)
    theta_full[pos] <- fixed_params[name]
  }
  
  y_sim <- simulate_y(X_train, theta_full, sample_params$sigma_sq)
  S_sim <- calc_summary_stats(y_sim)
  pilot_distances[i] <- distance_metric(S_sim, S_obs)
}

finite_dists <- pilot_distances[is.finite(pilot_distances)]
if (length(finite_dists) == 0) stop("No finite distances in pilot run!")

epsilon <- quantile(finite_dists, 0.05)
cat("Chosen epsilon (5th percentile):", epsilon, "\n")

# --- 7) Rejection ABC ---

accepted_thetas <- matrix(NA, nrow = N_sim, ncol = 2)
accepted_count <- 0

cat("Starting ABC rejection sampling...\n")
pb <- txtProgressBar(min = 0, max = N_sim, style = 3)

for (i in 1:N_sim) {
  sample_params <- prior_sampler()
  
  theta_full <- numeric(length(theta_hat_vec))
  theta_full[1] <- theta_hat_vec[1]
  theta_full[top2_idx[1] + 1] <- sample_params$theta_1
  theta_full[top2_idx[2] + 1] <- sample_params$theta_2
  for (name in fixed_names) {
    pos <- which(names(theta_hat_vec) == name)
    theta_full[pos] <- fixed_params[name]
  }
  
  y_sim <- simulate_y(X_train, theta_full, sample_params$sigma_sq)
  S_sim <- calc_summary_stats(y_sim)
  dist <- distance_metric(S_sim, S_obs)
  
  if (is.finite(dist) && dist < epsilon) {
    accepted_count <- accepted_count + 1
    accepted_thetas[accepted_count, ] <- c(sample_params$theta_1, sample_params$theta_2)
  }
  
  setTxtProgressBar(pb, i)
}
close(pb)

accepted_thetas <- accepted_thetas[1:accepted_count, , drop = FALSE]
cat("Number of accepted samples:", accepted_count, "\n")

# --- 8) Plot results ---

library(ggplot2)
library(gridExtra)

df_post <- data.frame(
  param1 = accepted_thetas[,1],
  param2 = accepted_thetas[,2]
)

p1 <- ggplot(df_post, aes(x = param1)) +
  geom_histogram(aes(y=..density..), bins = 30, fill = "skyblue", color = "black") +
  geom_vline(xintercept = theta_1_hat, color = "red", linetype = "dashed") +
  labs(title = paste("Posterior of", param_names[1]), x = param_names[1])

p2 <- ggplot(df_post, aes(x = param2)) +
  geom_histogram(aes(y=..density..), bins = 30, fill = "lightgreen", color = "black") +
  geom_vline(xintercept = theta_2_hat, color = "red", linetype = "dashed") +
  labs(title = paste("Posterior of", param_names[2]), x = param_names[2])

p_joint <- ggplot(df_post, aes(x = param1, y = param2)) +
  geom_point(alpha = 0.4, color = "blue") +
  geom_vline(xintercept = theta_1_hat, linetype = "dashed", color = "red") +
  geom_hline(yintercept = theta_2_hat, linetype = "dashed", color = "red") +
  labs(title = "Joint Posterior Distribution", x = param_names[1], y = param_names[2])

grid.arrange(p1, p2, p_joint, ncol = 2)


```

