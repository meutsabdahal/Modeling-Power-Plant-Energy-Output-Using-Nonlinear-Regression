# Installing Required Packages
```{r}
# install required packages/library
install.packages("matlib")
install.packages("rsample")
install.packages("ggplot2")
install.packages("corrplot")
install.packages("MASS")
install.packages("gridExtra")
install.packages("dplyr")
install.packages("knitr")

```

# Loading Libraries
```{r}
# import required packages/libraries
library(matlib)     # for matrix operations
library(ggplot2)    # for data visualization
library(rsample)    # for data splitting
library(corrplot)   # for correlation visualization
library(MASS)       # for statistical functions
library(gridExtra)  # for arranging plots
library(dplyr)      # for data manipulation
library(knitr)      # for tables
```

# Loading Datasets
```{r}
# load features dataset (independent variable)
features <- as.matrix(read.csv(file="data/features.csv", header=FALSE))
colnames(features) <- c("x1", "x3", "x4", "x5")

# load target dataset (dependent variable)
target <- as.matrix(read.csv(file="data/target.csv", header=FALSE))
colnames(target) <- c("X2")

# load time series dataset
time <- as.matrix((read.csv(file="data/timeseries.csv", header=FALSE)))
colnames(time) <- c("T1")

# display the first few rows of each dataset
cat("Features Dataset: \n")
head(features)

cat("\nTarget Dataset: \n")
head(target)

cat("\nTimeseries Dataset: \n")
head(features)
```

# Summary Statistics
```{r}
# create a combined dataframe
combined_df <- data.frame(
  Temperature = features[,"x1"],
  Ambient_Pressure = features[,"x3"],
  Relative_Humidity = features[,"x4"],
  Exhaust_Vacuum = features[,"x5"],
  Energy_Output = target[,"X2"]
)

# summary statistics
summary_stats <- summary(combined_df)
print(summary_stats)
```

# Task 1: Preliminary Data Analysis
## Time Series Analysis
### Time Series Analysis (Input Signal)
```{r}
# Function to plot time series with optional rolling average, and sampling at regular intervals
plot_time_series_enhanced <- function(time_series, var_name, xlab = "Time", ylab, 
                                     col = "black", 
                                     rolling_avg = FALSE, rolling_window = 100, 
                                     sample_interval = NULL) {
  
  time_series_to_plot <- time_series
  time_indices_to_plot <- seq_along(time_series)
  rolling_mean_to_plot <- NULL
  
  # 1. Rolling Average Calculation
  if (rolling_avg) {
    rolling_mean <- stats::filter(time_series, rep(1/rolling_window, rolling_window), sides = 2)
    
    # Trim NA values
    valid_indices <- rolling_window: (length(time_series) - rolling_window + 1)
    rolling_mean <- rolling_mean[valid_indices]
    time_series_to_plot <- time_series_to_plot[valid_indices]
    time_indices_to_plot <- time_indices_to_plot[valid_indices]
    rolling_mean_to_plot <- rolling_mean
  }
  
  # 2. Sampling at Intervals
  if (!is.null(sample_interval)) {
    sampled_indices <- seq(1, length(time_series_to_plot), by = sample_interval)
    time_series_to_plot <- time_series_to_plot[sampled_indices]
    time_indices_to_plot <- time_indices_to_plot[sampled_indices]
    if (rolling_avg) {
      rolling_mean_to_plot <- rolling_mean_to_plot[sampled_indices]
    }
  }
  
  # 3. Plotting
  plot(time_indices_to_plot, time_series_to_plot,
       type = "l",
       main = var_name,
       xlab = xlab,
       ylab = ylab,
       col = col,
       cex.lab = 1.2,
       cex.main = 1.4
  )
  
  # 4. Add Rolling Average Line (if requested)
  if (rolling_avg) {
    lines(time_indices_to_plot, rolling_mean_to_plot, col = "red", lwd = 2)
    legend("topright", legend = c("Original (or Sampled)", paste(rolling_window, "-Point Avg")),
           col = c(col, "red"), lty = 1, lwd = c(1, 2), cex = 0.8)
  }
}

# Convert features to time series objects
features.ts <- ts(features, start = c(min(time), max(time)), frequency = 1)

# Plotting with the new sampling interval
plot_time_series_enhanced(features.ts[, "x1"], "Temperature", ylab = "Temperature (°C)", col = "darkred", rolling_avg = TRUE, rolling_window = 100, sample_interval = 90)
plot_time_series_enhanced(features.ts[, "x3"], "Ambient Pressure", ylab = "Pressure (millibar)", col = "darkblue", rolling_avg = TRUE, rolling_window = 100, sample_interval = 90)
plot_time_series_enhanced(features.ts[, "x4"], "Relative Humidity", ylab = "Humidity (%)", col = "darkgreen", rolling_avg = TRUE, rolling_window = 100, sample_interval = 90)
plot_time_series_enhanced(features.ts[, "x5"], "Exhaust Vacuum", ylab = "Vacuum (cm Hg)", col = "darkorange", rolling_avg = TRUE, rolling_window = 100, sample_interval = 90)
```

### Time Series Analysis (Input Signal)
```{r}
# Function to plot target time series with optional rolling average, and sampling at regular intervals
plot_target_time_series_enhanced <- function(time_series, var_name, xlab = "Time", ylab,
                                            col = "purple", lwd = 1.5,
                                            rolling_avg = FALSE, rolling_window = 100,
                                            sample_interval = NULL) {

  time_series_to_plot <- time_series
  time_indices_to_plot <- seq_along(time_series)
  rolling_mean_to_plot <- NULL

  # 1. Rolling Average Calculation
  if (rolling_avg) {
    rolling_mean <- stats::filter(time_series, rep(1/rolling_window, rolling_window), sides = 2)

    # Trim NA values
    valid_indices <- rolling_window: (length(time_series) - rolling_window + 1)
    rolling_mean <- rolling_mean[valid_indices]
    time_series_to_plot <- time_series_to_plot[valid_indices]
    time_indices_to_plot <- time_indices_to_plot[valid_indices]
    rolling_mean_to_plot <- rolling_mean
  }

  # 2. Sampling at Intervals
  if (!is.null(sample_interval)) {
    sampled_indices <- seq(1, length(time_series_to_plot), by = sample_interval)
    time_series_to_plot <- time_series_to_plot[sampled_indices]
    time_indices_to_plot <- time_indices_to_plot[sampled_indices]
    if (rolling_avg) {
      rolling_mean_to_plot <- rolling_mean_to_plot[sampled_indices]
    }
  }

  # 3. Plotting
  plot(time_indices_to_plot, time_series_to_plot,
       type = "l",
       main = var_name,
       xlab = xlab,
       ylab = ylab,
       col = col,
       lwd = lwd,
       cex.lab = 1.2,
       cex.main = 1.4
  )

  # 4. Add Rolling Average Line (if requested)
  if (rolling_avg) {
    lines(time_indices_to_plot, rolling_mean_to_plot, col = "red", lwd = 2)
    legend("topright", legend = c("Original (or Sampled)", paste(rolling_window, "-Point Avg")),
           col = c(col, "red"), lty = 1, lwd = c(1, 2), cex = 0.8)
  }
}

# Convert target to time series object
target.ts <- ts(target, start = c(min(time), max(time)), frequency = 1)

# Plotting the target time series with options
plot_target_time_series_enhanced(target.ts, "Net Hourly Electrical Energy Output",
                                 xlab = "Time", ylab = "Energy Output (MW)",
                                 col = "purple", lwd = 1.5,
                                 rolling_avg = TRUE, rolling_window = 100,
                                 sample_interval = 90)
```

## Distribution of Each Signal (Output Signal)
```{r}
# function to create enhanced histogram with density plot and normal reference
plot_enhanced_histogram <- function(data, column, title, xlab) {
  # calculate density
  dens <- density(data)
  
  # create histogram
  hist(data, freq = FALSE, main = title, xlab = xlab, col = "lightblue",
    border = "white", ylim = c(0, max(dens$y) * 1.1))
  
  # add density line
  lines(dens, lwd = 2, col = "darkblue")
  
  # add normal distribution reference
  curve(dnorm(x, mean = mean(data), sd = sd(data)), add = TRUE,
    col = "red", lty = 2, lwd = 1.5)
  
  # add legend
  legend("topleft", legend = c("Density", "Normal Reference"),
    col = c("darkblue", "red"), lty = c(1, 2), lwd = c(2, 1.5))
  
  # add mean line
  abline(v = mean(data), col = "darkgreen", lwd = 1.5, lty = 2)
  
  # add text for mean and standard deviation
  usr <- par("usr")  # c(x1, x2, y1, y2)
  text(x = usr[2] - 0.02 * diff(usr[1:2]), y = usr[4] - 0.05 * diff(usr[3:4]),  
    labels = sprintf("Mean: %.2f\nSD:   %.2f", mean(data), sd(data)), adj = c(1, 1))
}

# histogram for temperature (x1)
plot_enhanced_histogram(
  features[,"x1"], 
  "x1", 
  "Distribution of Temperature (x1)", 
  "Ambient temperature (°C)"
)

# histogram for ambient pressure (x3)
plot_enhanced_histogram(
  features[,"x3"], 
  "x3", 
  "Distribution of Ambient Pressure (x3)", 
  "Atmospheric pressure (millibar)"
)

# histogram for relative humidity (x4)
plot_enhanced_histogram(
  features[,"x4"], 
  "x4", 
  "Distribution of Relative Humidity (x4)", 
  "Humidity level (%)"
)

# histogram for exhaust vacuum (x5)
plot_enhanced_histogram(
  features[,"x5"], 
  "x5", 
  "Distribution of Exhaust Vacuum (x5)", 
  "Vacuum (cm Hg)"
)

# histogram for net hourly electrical energy output (X2)
plot_enhanced_histogram(
  target[,"X2"], 
  "X2", 
  "Distribution of Net Hourly Electrical Energy Output (X2)", 
  "Net hourly electrical energy output (MW)"
)

```

## Correlation Analysis and Scatter Plots
### Correlation Matrix
```{r}
# combine features and target for correlation analysis
combined <- cbind(features, target)

# calculate correlation matrix
correlation <- cor(combined)
print(correlation)

# create enhanced correlation plot
corrplot(correlation, method = "color", type = "upper", 
         addCoef.col = "black", number.cex = 0.7,
         tl.col = "black", tl.srt = 45,
         title = "Correlation Matrix of Power Plant Variables",
         mar = c(0, 0, 1, 0))
```

### Scatter Plots
```{r}
# Define a palette of 4 colors (one per feature)
base_colors <- c("steelblue", "tomato", "forestgreen", "orchid")
# Optionally add a bit of transparency
plot_colors <- sapply(base_colors, function(col) adjustcolor(col, alpha.f = 0.7))

# List of columns and their labels
plots <- list(
  x1 = c("Ambient Temp (°C)",       "Net Energy Output (MW)"),
  x3 = c("Ambient Pressure (mbar)", "Net Energy Output (MW)"),
  x4 = c("Relative Humidity (%)",   "Net Energy Output (MW)"),
  x5 = c("Exhaust Vacuum (cm Hg)",  "Net Energy Output (MW)")
)

# Reset to single plot (ensure only one plot is drawn at a time)
par(mfrow = c(1, 1), mar = c(4, 4, 2, 1))

# Loop over each feature → plot(feature, target) with its own color
for (name in names(plots)) {
  labs <- plots[[name]]
  x    <- features[, name]
  y    <- target[, "X2"]
  
  plot(
    x, y,
    xlab  = labs[1],
    ylab  = labs[2],
    main  = paste("Scatter Plot:", labs[1], "vs", labs[2]),
    pch   = 16,
    cex   = 0.6,
    col   = plot_colors[which(names(plots) == name)]  # Correct color selection
  )
}

```

# Task 2: Regression Modeling
## Task 2.1: Parameter Estimation Using Ridge Regression

```{r}
lambda_ols_param <- 0     # Setting lambda to 0 for Ordinary Least Squares (no regularization)

# Function to estimate theta using OLS with scaled predictors
estimate_theta_ols_scaled <- function(X_scaled_no_intercept, y_response_vec, lambda_val) {
  # Handle intercept-only model (if X_scaled_no_intercept has columns but no rows, or no columns)
  if (ncol(X_scaled_no_intercept) == 0 && nrow(X_scaled_no_intercept) > 0) {
    X_final_mat <- matrix(1, nrow = nrow(X_scaled_no_intercept), ncol = 1)     # Design matrix for intercept-only
    colnames(X_final_mat) <- c("(Intercept)")
  } else if (nrow(X_scaled_no_intercept) > 0) {     # Model with predictors
    X_final_mat <- cbind("(Intercept)" = rep(1, nrow(X_scaled_no_intercept)), X_scaled_no_intercept)     # Add intercept column
  } else {     # No data
    warning("Input X_scaled_no_intercept has 0 rows.")
    return(NA)     # Return NA if no data
  }

  # Check if there are enough data points for the number of parameters
  if (nrow(X_final_mat) < ncol(X_final_mat)) {
    warning(paste0("Skipping OLS - not enough rows (", nrow(X_final_mat),
                  ") for predictors (", ncol(X_final_mat), ")."))
    return(NA)     # Return NA if not enough data
  }

  # Calculate theta_hat using the OLS formula with potential regularization (lambda_val)
  theta_hat <- tryCatch({
    XtX <- t(X_final_mat) %*% X_final_mat     # X transpose X
    penalty_matrix <- lambda_val * diag(ncol(X_final_mat))     # Penalty matrix for Ridge regression (0 if lambda_val=0)
    if (ncol(X_final_mat) > 0) penalty_matrix[1,1] <- 0     # Intercept is typically not penalized
    XtX_reg <- XtX + penalty_matrix     # Regularized XtX (or just XtX if lambda_val is 0)
    
    # Use Moore-Penrose generalized inverse (ginv) for numerical stability
    estimated_coeffs <- MASS::ginv(XtX_reg) %*% t(X_final_mat) %*% y_response_vec
    rownames(estimated_coeffs) <- colnames(X_final_mat)     # Assign names to coefficients
    return(estimated_coeffs)
  }, error = function(e) {
    message(paste0("Error calculating theta_hat with OLS: ", e$message))
    return(NA)     # Return NA on error
  })
  return(theta_hat)
}

# Initialize lists to store results for all models
theta_list_all_models <- list()     # To store estimated parameters (theta_hat)
X_final_processed_list_all_models <- list()     # To store the design matrix (X with intercept and scaled predictors)

# Define model definitions  
# model 1: uses relative humidity and squared ambient pressure
# model 2: extends Model 1 by adding exhaust vacuum
# model 3: uses ambient pressure, relative humidity, and cubed exhaust vacuum
# model 4: uses relative humidity, squared ambient pressure, and cubed exhaust vacuum
# model 5: uses relative humidity, squared temperature, and squared ambient pressure
model_definitions <- list(
  Model1 = data.frame(x4 = features[,"x4"], x3 = features[,"x3"]^2),
  Model2 = data.frame(x4 = features[,"x4"], x3 = features[,"x3"]^2, x5 = features[,"x5"]),
  Model3 = data.frame(x3 = features[,"x3"], x4 = features[,"x4"], x5 = features[,"x5"]^3),
  Model4 = data.frame(x4 = features[,"x4"], x3 = features[,"x3"]^2, x5 = features[,"x5"]^3),
  Model5 = data.frame(x4 = features[,"x4"], x1 = features[,"x1"]^2, x3 = features[,"x3"]^2)
)
# Loop through each model definition
for (model_label in names(model_definitions)) {
  cat("\nTheta for", model_label, "\n")
  X_raw_current_df <- model_definitions[[model_label]]     # Get raw predictors for the current model

  # Skip if model data is null or empty
  if (is.null(X_raw_current_df) || nrow(X_raw_current_df) == 0) {
    theta_list_all_models[[model_label]] <- NA
    X_final_processed_list_all_models[[model_label]] <- NA
    next
  }
  
  X_raw_current_mat <- as.matrix(X_raw_current_df)     # Convert to matrix
  X_scaled_current_no_int <- matrix(nrow = nrow(X_raw_current_mat), ncol = 0)     # Initialize scaled matrix (no intercept yet)

  # Scale predictors if there are any
  if (ncol(X_raw_current_mat) > 0) {
    original_colnames <- colnames(X_raw_current_mat)     # Store original column names
    # Assign default column names if missing
    if (is.null(original_colnames)) {
        original_colnames <- if(ncol(X_raw_current_mat) == 1 && !is.null(names(X_raw_current_df))) names(X_raw_current_df) else paste0("V", 1:ncol(X_raw_current_mat))
    }
    X_scaled_temp_mat <- scale(X_raw_current_mat)     # Scale the predictors (center and scale)
    X_scaled_temp_mat[is.na(X_scaled_temp_mat)] <- 0     # Replace any NA/NaN from scaling (e.g., constant column) with 0
    X_scaled_current_no_int <- X_scaled_temp_mat
    colnames(X_scaled_current_no_int) <- original_colnames     # Restore column names
  }

  # Estimate theta_hat for the current model using scaled predictors
  current_theta_hat_val <- estimate_theta_ols_scaled(X_scaled_current_no_int, target, lambda_val = lambda_ols_param)
  theta_list_all_models[[model_label]] <- current_theta_hat_val     # Store estimated theta

  # Store the final processed design matrix (with intercept and scaled predictors)
  if (!is.null(current_theta_hat_val) && !all(is.na(current_theta_hat_val))) {
      X_final_processed_list_all_models[[model_label]] <- if (ncol(X_scaled_current_no_int) == 0 && nrow(X_scaled_current_no_int) > 0) {
          # Design matrix for intercept-only model
          matrix(1, nrow = nrow(X_scaled_current_no_int), ncol = 1, dimnames = list(NULL, "(Intercept)"))
      } else if (nrow(X_scaled_current_no_int) > 0) {
          # Design matrix with intercept and scaled predictors
          cbind("(Intercept)" = rep(1, nrow(X_scaled_current_no_int)), X_scaled_current_no_int)
      } else { NA }
  } else {
      X_final_processed_list_all_models[[model_label]] <- NA     # Store NA if theta estimation failed
  }
  print(current_theta_hat_val)
}
```

## Task 2.2: Model Residual Sum of Squared Error (RSS)
```{r}
# Function to compute RSS using design matrix and parameters
compute_rss_matrix <- function(X, y, theta) {
  # Predict y values using Xθ̂
  y_pred <- tryCatch(X %*% theta, error = function(e) NULL)
  if (is.null(y_pred)) return(NA)   # Return NA if prediction fails
  
  # Compute RSS
  sum((y - y_pred)^2, na.rm = TRUE) # Sum of squared differences, remove NA if any
}

# Prepare design matrices for RSS calculation (including intercept)
# Use the X_final_processed_list_all_models directly!
rss_values <- lapply(names(model_definitions), function(model_name) {
  X_matrix <- X_final_processed_list_all_models[[model_name]]
  theta <- theta_list_all_models[[model_name]]
  
  # Check for NA or NULL and dimension compatibility
  if (is.null(X_matrix) || any(is.na(X_matrix)) || is.null(theta) || any(is.na(theta)) || ncol(X_matrix) != length(theta)) {
    return(NA)   # Return NA if there are issues
  }
  
  compute_rss_matrix(X_matrix, target, theta)
})
names(rss_values) <- names(model_definitions)

# Print the RSS values in the desired format
for (model_name in names(rss_values)) {
  cat(model_name, "RSS: ", rss_values[[model_name]], "\n")
}
```

## Task 2.3: Log-likelihood and Variance Calculation
```{r}
# define function to compute log-likelihood and variance
compute_loglikelihood_variance <- function(rss, n) {
  # estimate error variance from RSS
  sigma_squared <- rss / (n - 1)
  
  # compute log-likelihood under Gaussian error assumption
  log_likelihood <- -(n/2) * log(2 * pi) - (n/2) * log(sigma_squared) - 
                   (1/(2 * sigma_squared)) * rss
  
  return(list(log_likelihood = log_likelihood, variance = sigma_squared))
}

# calculate log-likelihood and variance for each model
n_samples <- length(target)  # number of observations

loglik_var_list_all_models <- lapply(names(model_definitions), function(model_name) {
  # Get RSS for the model
  if (model_name == "Model1") {
    rss_val <- rss_values[["Model1"]]
  } else if (model_name == "Model2") {
    rss_val <- rss_values[["Model2"]]
  } else if (model_name == "Model3") {
    rss_val <- rss_values[["Model3"]]
  } else if (model_name == "Model4") {
    rss_val <- rss_values[["Model4"]]
  } else if (model_name == "Model5") {
    rss_val <- rss_values[["Model5"]]
  } else {
    return(list(log_likelihood = NA, variance = NA)) # Handle unexpected model name
  }

  if (is.na(rss_val)) {
    return(list(log_likelihood = NA, variance = NA)) # Return NA if RSS is NA
  }

  results <- compute_loglikelihood_variance(rss_val, n_samples)
  return(results)
})
names(loglik_var_list_all_models) <- names(model_definitions) # Assign model names


# Display the results in your preferred format
for (model_name in names(loglik_var_list_all_models)) {
  results <- loglik_var_list_all_models[[model_name]]
  cat("Model ", gsub("Model", "", model_name), ":\n", sep = "") # Display "Model 1" instead of "Model1"
  cat("Log-likelihood: ", results$log_likelihood, "\n", sep = "")
  cat("Variance: ", results$variance, "\n\n", sep = "")
}
```

## Task 2.4: Compute AIC and BIC
```{r}
# define functions to compute AIC and BIC
compute_aic <- function(log_likelihood, k) {
  aic <- 2 * k - 2 * log_likelihood
  return(aic)
}

compute_bic <- function(log_likelihood, k, n) {
  bic <- k * log(n) - 2 * log_likelihood
  return(bic)
}

# Calculate AIC and BIC for each model
aic_bic_list_all_models <- lapply(names(model_definitions), function(model_name) {
  # Get log-likelihood for the current model
  log_likelihood <- loglik_var_list_all_models[[model_name]]$log_likelihood
  
  # Determine the number of parameters (k) for the current model
  # Assuming theta_list_all_models contains the parameter vectors
  k <- length(theta_list_all_models[[model_name]][!is.na(theta_list_all_models[[model_name]])])
  
  # Get the number of observations (n)
  n <- length(target)  # Assuming 'target' is your target variable
  
  # Calculate AIC and BIC
  aic <- compute_aic(log_likelihood, k)
  bic <- compute_bic(log_likelihood, k, n)
  
  return(list(AIC = aic, BIC = bic))
})

# Assign names to the results list
names(aic_bic_list_all_models) <- names(model_definitions)

# Print the AIC and BIC for each model
for (model_name in names(aic_bic_list_all_models)) {
  cat("Model ", gsub("Model", "", model_name), ":\n", sep = "")
  cat("  AIC: ", aic_bic_list_all_models[[model_name]]$AIC, "\n", sep = "")
  cat("  BIC: ", aic_bic_list_all_models[[model_name]]$BIC, "\n\n", sep = "")
}
```


## Task 2.5: Distribution of Model Prediction Errors
```{r}
# define function to compute model prediction errors (residuals)
compute_model_prediction_error <- function(X, y, theta, model_func) {
  # generate predictions using the model
  # Handle cases where X might be NULL or have zero rows or columns
  if (is.null(X) || nrow(X) == 0 || ncol(X) == 0) {
    y_pred <- rep(theta[1], length(y))  # Predict using only the intercept
  } else {
    X <- as.matrix(X)
    # Ensure X has an intercept column; if not, add it
    if (!("(Intercept)" %in% colnames(X))) {
      X <- cbind("(Intercept)" = rep(1, nrow(X)), X)
    }
    y_pred <- X %*% theta
  }
  
  # calculate prediction errors/residuals
  prediction_error <- y - y_pred
  
  return(prediction_error)
}

# Define model functions (these should match your model definitions)
# Ensure these functions handle cases with no predictors gracefully
model1_func <- function(X, theta) {
  if (is.null(X) || nrow(X) == 0 || ncol(X) == 0) {
    return(rep(theta[1], nrow(target)))  # Intercept-only model
  } else {
    X <- as.matrix(X)
    # Ensure X has an intercept column; if not, add it
    if (!("(Intercept)" %in% colnames(X))) {
      X <- cbind("(Intercept)" = rep(1, nrow(target)), X)
    }
    return(X %*% theta)
  }
}

model2_func <- function(X, theta) {
  if (is.null(X) || nrow(X) == 0 || ncol(X) == 0) {
    return(rep(theta[1], nrow(target)))  # Intercept-only model
  } else {
    X <- as.matrix(X)
    # Ensure X has an intercept column; if not, add it
    if (!("(Intercept)" %in% colnames(X))) {
      X <- cbind("(Intercept)" = rep(1, nrow(target)), X)
    }
    return(X %*% theta)
  }
}

model3_func <- function(X, theta) {
  if (is.null(X) || nrow(X) == 0 || ncol(X) == 0) {
    return(rep(theta[1], nrow(target)))  # Intercept-only model
  } else {
    X <- as.matrix(X)
    # Ensure X has an intercept column; if not, add it
    if (!("(Intercept)" %in% colnames(X))) {
      X <- cbind("(Intercept)" = rep(1, nrow(target)), X)
    }
    return(X %*% theta)
  }
}

model4_func <- function(X, theta) {
  if (is.null(X) || nrow(X) == 0 || ncol(X) == 0) {
    return(rep(theta[1], nrow(target)))  # Intercept-only model
  } else {
    X <- as.matrix(X)
    # Ensure X has an intercept column; if not, add it
    if (!("(Intercept)" %in% colnames(X))) {
      X <- cbind("(Intercept)" = rep(1, nrow(target)), X)
    }
    return(X %*% theta)
  }
}

model5_func <- function(X, theta) {
  if (is.null(X) || nrow(X) == 0 || ncol(X) == 0) {
    return(rep(theta[1], nrow(target)))  # Intercept-only model
  } else {
    X <- as.matrix(X)
    # Ensure X has an intercept column; if not, add it
    if (!("(Intercept)" %in% colnames(X))) {
      X <- cbind("(Intercept)" = rep(1, nrow(target)), X)
    }
    return(X %*% theta)
  }
}

# Calculate prediction errors for each model
prediction_error_model1_vec <- compute_model_prediction_error(
  X_final_processed_list_all_models[["Model1"]], target, theta_list_all_models[["Model1"]], model1_func
)

prediction_error_model2_vec <- compute_model_prediction_error(
  X_final_processed_list_all_models[["Model2"]], target, theta_list_all_models[["Model2"]], model2_func
)

prediction_error_model3_vec <- compute_model_prediction_error(
  X_final_processed_list_all_models[["Model3"]], target, theta_list_all_models[["Model3"]], model3_func
)

prediction_error_model4_vec <- compute_model_prediction_error(
  X_final_processed_list_all_models[["Model4"]], target, theta_list_all_models[["Model4"]], model4_func
)

prediction_error_model5_vec <- compute_model_prediction_error(
  X_final_processed_list_all_models[["Model5"]], target, theta_list_all_models[["Model5"]], model5_func
)


# create enhanced QQ plots to check for normality of residuals
# function to create enhanced QQ plot
create_enhanced_qq <- function(residuals, model_name) {
  # generate QQ plot
  qqnorm(residuals, main = paste("Q-Q Plot of Residuals for", model_name),
         col = "blue", pch = 19, cex = 0.5)
  qqline(residuals, col = "red", lwd = 2)
  
}

# create QQ plots for all models
create_enhanced_qq(prediction_error_model1_vec, "Model 1")
create_enhanced_qq(prediction_error_model2_vec, "Model 2")
create_enhanced_qq(prediction_error_model3_vec, "Model 3")
create_enhanced_qq(prediction_error_model4_vec, "Model 4")
create_enhanced_qq(prediction_error_model5_vec, "Model 5")
```

```{r}
# compile all model evaluation metrics into a comprehensive table
model_comparison <- data.frame(
  Model = names(model_definitions),
  Parameters = sapply(theta_list_all_models, function(theta) length(theta[!is.na(theta)])),
  RSS = sapply(rss_values, function(rss) ifelse(is.null(rss) || length(rss) == 0 || is.na(rss), NA, rss)),
  LogLikelihood = sapply(loglik_var_list_all_models, function(item) ifelse(is.null(item) || is.na(item$log_likelihood), NA, item$log_likelihood)),
  AIC = sapply(aic_bic_list_all_models, function(item) ifelse(is.null(item) || is.na(item$AIC), NA, item$AIC)),
  BIC = sapply(aic_bic_list_all_models, function(item) ifelse(is.null(item) || is.na(item$BIC), NA, item$BIC)),
  Variance = sapply(loglik_var_list_all_models, function(item) ifelse(is.null(item) || is.na(item$variance), NA, item$variance))
)

# display the comprehensive table
kable(model_comparison, 
      caption = "Comprehensive Comparison of All Models", 
      digits = c(0, 0, 2, 2, 2, 2, 4))

# rank models based on different criteria
rank_rss <- rank(model_comparison$RSS, na.last = "keep")
rank_aic <- rank(model_comparison$AIC, na.last = "keep")
rank_bic <- rank(model_comparison$BIC, na.last = "keep")
rank_loglik <- rank(-model_comparison$LogLikelihood, na.last = "keep")  # Negative because higher is better

ranking_table <- data.frame(
  Model = model_comparison$Model,
  RSS_Rank = rank_rss,
  AIC_Rank = rank_aic,
  BIC_Rank = rank_bic,
  LogLikelihood_Rank = rank_loglik,
  Overall_Rank = rank_rss + rank_aic + rank_bic + rank_loglik
)

# sort by overall rank (lower is better)
ranking_table <- ranking_table[order(ranking_table$Overall_Rank),]

# display the ranking table
kable(ranking_table, 
      caption = "Model Ranking Based on Different Criteria (Lower is Better)")

# determine the best model based on our criteria
best_model_aic_index <- which.min(model_comparison$AIC)
best_model_aic_name <- model_comparison$Model[best_model_aic_index]

best_model_bic_index <- which.min(model_comparison$BIC)
best_model_bic_name <- model_comparison$Model[best_model_bic_index]

best_model_loglik_index <- which.max(model_comparison$LogLikelihood)
best_model_loglik_name <- model_comparison$Model[best_model_loglik_index]

best_model_rss_index <- which.min(model_comparison$RSS)
best_model_rss_name <- model_comparison$Model[best_model_rss_index]


cat(paste("Based on AIC, the best model is:", best_model_aic_name, "\n"))
cat(paste("Based on BIC, the best model is:", best_model_bic_name, "\n"))
cat(paste("Based on Log-Likelihood, the best model is:", best_model_loglik_name, "\n"))
cat(paste("Based on RSS, the best model is:", best_model_rss_name, "\n"))
```

## Train-Test Split
```{r}
set.seed(123)

# create indices for 70% train, 30% test split
sample_size <- nrow(features)
train_indices <- sample(sample_size, size = round(0.7 * sample_size))
test_indices <- setdiff(1:sample_size, train_indices)

# create training and testing datasets
train_features <- features[train_indices, ]
train_target <- target[train_indices, ]
test_features <- features[test_indices, ]
test_target <- target[test_indices, ]

train_X <- data.frame(x4 = train_features[,"x4"], 
                     x1_squared = train_features[,"x1"]^2, 
                     x3_squared = train_features[,"x3"]^2)
                     
test_X <- data.frame(x4 = test_features[,"x4"], 
                    x1_squared = test_features[,"x1"]^2, 
                    x3_squared = test_features[,"x3"]^2)

# Scale the training data (excluding intercept if it exists)
train_X_scaled <- scale(train_X)
train_X_scaled_no_intercept <- train_X_scaled[, !(colnames(train_X_scaled) %in% "(Intercept)")]

# estimate parameters using training data
train_theta <- estimate_theta_ols_scaled(train_X_scaled_no_intercept, train_target, lambda_ols_param)

cat("Model 5 parameters estimated on training data:\n")
print(train_theta)

# calculate predictions on test data
test_predictions <- model5(test_X, train_theta)

# calculate test error metrics
test_rss <- sum((test_target - test_predictions)^2)
test_rmse <- sqrt(mean((test_target - test_predictions)^2))
test_mae <- mean(abs(test_target - test_predictions))
test_r_squared <- 1 - sum((test_target - test_predictions)^2) / sum((test_target - mean(test_target))^2)

# display test metrics
test_metrics <- data.frame(
  Metric = c("RSS", "RMSE", "MAE", "R-squared"),
  Value = c(test_rss, test_rmse, test_mae, test_r_squared)
)

kable(test_metrics, caption = "Model Performance on Test Data")
```

### Calculate Confidence Intervals and Visualization
```{r}
# function to compute confidence intervals for predictions
compute_confidence_intervals <- function(predictions, variance, confidence_level = 0.95) {
  # number of observations
  n_obs <- length(predictions)
  
  # calculate margin of error for the confidence interval
  alpha <- 1 - confidence_level
  t_critical <- qt(1 - alpha/2, df = n_obs - 4)  # 4 parameters in Model 5
  margin_error <- t_critical * sqrt(variance)
  
  # return lower and upper bounds
  return(list(
    lower = predictions - margin_error,
    upper = predictions + margin_error
  ))
}

# compute the variance of model prediction errors from training data
model5_prediction_errors <- train_target - model5(train_X, train_theta)
model5_prediction_variance <- var(model5_prediction_errors)

# compute 95% confidence intervals
ci <- compute_confidence_intervals(test_predictions, model5_prediction_variance, 0.95)

# create dataframe for plotting
plot_data <- data.frame(
  Observation = 1:length(test_predictions),
  Actual = test_target,
  Predicted = test_predictions,
  Lower_CI = ci$lower,
  Upper_CI = ci$upper
)

# create visualization of predictions with confidence intervals
ggplot(plot_data, aes(x = Observation)) +
  # plot actual observations
  geom_point(aes(y = Actual), color = "blue", alpha = 0.5, size = 1.5) +
  # plot predicted values
  geom_line(aes(y = Predicted), color = "red", size = 1) +
  # add confidence interval ribbon
  geom_ribbon(aes(ymin = Lower_CI, ymax = Upper_CI), alpha = 0.2, fill = "gray") +
  # customize plot appearance
  theme_minimal() +
  labs(
    title = "Test Data Predictions with 95% Confidence Intervals - Model 5",
    x = "Observation Index",
    y = "Net Hourly Electrical Energy Output (MW)"
  ) +
  theme(
    plot.title = element_text(face = "bold"),
    axis.title = element_text(face = "bold")
  )
```

# Task 3: Approximate Bayesian Computation (ABC)
```{r}
# two parameters with largest absolute values in Model 5
param_abs_values <- abs(train_theta)

intercept_index <- 4
param_abs_values[intercept_index] <- 0  # Exclude intercept from consideration

# find indices of two largest parameter values
top_param_indices <- order(param_abs_values, decreasing = TRUE)[1:2]
top_param_values <- train_theta[top_param_indices]

cat("The two parameters with largest absolute values are at positions", 
    top_param_indices, "with values", top_param_values, "\n")

# define bounds for uniform prior
prior_bounds <- matrix(0, nrow = 2, ncol = 2)
for(i in 1:2) {
  param_value <- top_param_values[i]
  prior_bounds[i, 1] <- param_value - 0.5 * abs(param_value)  # Lower bound
  prior_bounds[i, 2] <- param_value + 0.5 * abs(param_value)  # Upper bound
}

cat("Prior bounds for the two parameters:\n")
print(prior_bounds)
```

## Uniform Distribution
```{r}
# function to simulate data from Model 5 with given parameters
simulate_data <- function(X, theta_modified, orig_theta, param_indices) {
  # create a copy of the original theta
  theta_copy <- orig_theta
  
  # modify only the two selected parameters
  theta_copy[param_indices] <- theta_modified
  
  # generate predictions using Model 5 with modified parameters
  simulated_y <- model5(X, theta_copy)
  
  # add Gaussian noise based on estimated variance
  noise_sd <- sqrt(model5_prediction_variance)
  simulated_y <- simulated_y + rnorm(length(simulated_y), mean = 0, sd = noise_sd)
  
  return(simulated_y)
}

# function to compute distance between observed and simulated data
compute_distance <- function(observed, simulated) {
  # using RMSE as the distance metric
  return(sqrt(mean((observed - simulated)^2)))
}

# rejection ABC algorithm
rejection_abc <- function(X, observed_y, orig_theta, param_indices, prior_bounds, 
                          n_samples = 5000, acceptance_rate = 0.05) {
  # number of samples to accept
  n_accept <- round(n_samples * acceptance_rate)
  
  # storage for samples and distances
  all_samples <- matrix(0, nrow = n_samples, ncol = 2)
  all_distances <- rep(0, n_samples)
  
  # generate samples from prior
  for(i in 1:n_samples) {
    # sample from uniform prior for the two parameters
    theta_sample <- c(
      runif(1, prior_bounds[1, 1], prior_bounds[1, 2]),
      runif(1, prior_bounds[2, 1], prior_bounds[2, 2])
    )
    
    # simulate data with sampled parameters
    simulated_y <- simulate_data(X, theta_sample, orig_theta, param_indices)
    
    # compute distance
    distance <- compute_distance(observed_y, simulated_y)
    
    # store sample and distance
    all_samples[i, ] <- theta_sample
    all_distances[i] <- distance
  }
  
  # select samples with smallest distances
  accepted_indices <- order(all_distances)[1:n_accept]
  accepted_samples <- all_samples[accepted_indices, ]
  accepted_distances <- all_distances[accepted_indices]
  
  return(list(
    samples = accepted_samples,
    distances = accepted_distances,
    all_samples = all_samples,
    all_distances = all_distances
  ))
}

# ABC
set.seed(789)
abc_results <- rejection_abc(
  test_X, 
  test_target, 
  train_theta, 
  top_param_indices, 
  prior_bounds,
  n_samples = 5000, 
  acceptance_rate = 0.05
)

# display summary of accepted samples
cat("Summary of accepted parameter samples:\n")
print(summary(abc_results$samples))

cat("Mean accepted distance:", mean(abc_results$distances), "\n")
```

## Visualizing ABC Results
```{r}
# create data frames for visualization
accepted_df <- data.frame(
  Param1 = abc_results$samples[, 1],
  Param2 = abc_results$samples[, 2],
  Distance = abc_results$distances
)

# joint posterior distribution
ggplot(accepted_df, aes(x = Param1, y = Param2)) +
  geom_point(alpha = 0.5, color = "blue") +
  geom_point(data = data.frame(
    Param1 = train_theta[top_param_indices[1]],
    Param2 = train_theta[top_param_indices[2]]
  ), color = "red", size = 5, shape = "+") +
  labs(
    title = "Joint Posterior Distribution from ABC",
    x = paste("Parameter", top_param_indices[1]),
    y = paste("Parameter", top_param_indices[2])
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold"),
    axis.title = element_text(face = "bold")
  )

# marginal posterior distributions

# first parameter
hist(abc_results$samples[, 1], breaks = 20, 
     main = paste("Marginal Posterior for Parameter", top_param_indices[1]),
     xlab = paste("Parameter", top_param_indices[1]),
     col = "skyblue", border = "white")
abline(v = train_theta[top_param_indices[1]], col = "red", lwd = 2, lty = 2)

# second parameter
hist(abc_results$samples[, 2], breaks = 20, 
     main = paste("Marginal Posterior for Parameter", top_param_indices[2]),
     xlab = paste("Parameter", top_param_indices[2]),
     col = "lightgreen", border = "white")
abline(v = train_theta[top_param_indices[2]], col = "red", lwd = 2, lty = 2)

```

## Joint and Marginal Posterior Distribution
```{r}
# calculate 95% credible intervals for the parameters
ci_param1 <- quantile(abc_results$samples[, 1], c(0.025, 0.975))
ci_param2 <- quantile(abc_results$samples[, 2], c(0.025, 0.975))

cat("95% Credible interval for Parameter", top_param_indices[1], ":", ci_param1, "\n")
cat("95% Credible interval for Parameter", top_param_indices[2], ":", ci_param2, "\n")

# create density plots with credible intervals

# first parameter density
plot(density(abc_results$samples[, 1]), 
     main = paste("Parameter", top_param_indices[1], "Posterior"),
     xlab = paste("Parameter", top_param_indices[1]),
     col = "blue", lwd = 2)
abline(v = train_theta[top_param_indices[1]], col = "red", lwd = 2, lty = 2)
abline(v = ci_param1, col = "darkblue", lwd = 1, lty = 3)
legend("topright", 
       legend = c("Posterior Density", "Point Estimate", "95% Credible Interval"),
       col = c("blue", "red", "darkblue"), 
       lty = c(1, 2, 3), 
       lwd = c(2, 2, 1))

# second parameter density
plot(density(abc_results$samples[, 2]), 
     main = paste("Parameter", top_param_indices[2], "Posterior"),
     xlab = paste("Parameter", top_param_indices[2]),
     col = "green", lwd = 2)
abline(v = train_theta[top_param_indices[2]], col = "red", lwd = 2, lty = 2)
abline(v = ci_param2, col = "darkgreen", lwd = 1, lty = 3)
legend("topright", 
       legend = c("Posterior Density", "Point Estimate", "95% Credible Interval"),
       col = c("green", "red", "darkgreen"), 
       lty = c(1, 2, 3), 
       lwd = c(2, 2, 1))


```

